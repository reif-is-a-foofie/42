# 42.un Development Tasks

*Following MASTERPLAN.md phases, V.zero/TASKS.md structure, and .cursor-rules discipline*

## ðŸŽ¯ **Current Status: Phase un - Reflex and ingestion (COMPLETED)**
- âœ… Redis event bus working
- âœ… 42.un scanning for sources  
- âœ… Task prioritization working
- âœ… Test suite in place
- âœ… Embeddings fixed and working (78 chunks, 62 unique URLs)

## ðŸš€ **Next Phase: Phase deux - Advanced optimization**

### **Task 1: Clustering Engine** 
*Estimated: 4 hours | Timeout: 8 hours*

**Following TASKS.md: Clustering Engine section**

- [ ] **Create `42/cluster.py`** with HDBSCAN wrapper
  - [ ] Implement `recluster_vectors() -> Dict[str, Any]`
  - [ ] Load all vectors from Qdrant
  - [ ] Run HDBSCAN clustering with proper parameters
  - [ ] Update payloads with `cluster_id`
  - [ ] Generate UMAP visualization saved to `docs/cluster.png`
  - [ ] Return cluster statistics

- [ ] **Add CLI command** `42 recluster`
  - [ ] Call FastAPI route via HTTP
  - [ ] Show progress feedback
  - [ ] Display cluster statistics

- [ ] **Add FastAPI endpoint** `/recluster`
  - [ ] Use Pydantic models for request/response
  - [ ] Return cluster statistics
  - [ ] Handle errors gracefully

- [ ] **Write tests** in `tests/test_cluster.py`
  - [ ] Mock vector store for predictable results
  - [ ] Test with small set of vectors
  - [ ] Assert clusters are returned correctly
  - [ ] Test CLI command with typer testing utilities

**Dependencies**: `hdbscan`, `umap-learn` (pinned versions in requirements.txt)

---

### **Task 2: Prompt Builder**
*Estimated: 3 hours | Timeout: 6 hours*

**Following TASKS.md: Prompt Builder section**

- [ ] **Create `42/prompt.py`** with prompt building logic
  - [ ] Implement `build_prompt(question: str, top_n: int = 5) -> str`
  - [ ] Fetch top-N similar vectors from Qdrant
  - [ ] Insert chunks into template
  - [ ] Trim to model token limit
  - [ ] Return formatted prompt

- [ ] **Add CLI command** `42 ask --question TEXT`
  - [ ] Build prompt from question
  - [ ] Call LLM engine
  - [ ] Display response

- [ ] **Add FastAPI endpoint** `/ask`
  - [ ] Use Pydantic models for request/response
  - [ ] Return LLM response
  - [ ] Handle streaming responses

- [ ] **Write tests** in `tests/test_prompt.py`
  - [ ] Mock vector store for predictable prompts
  - [ ] Test top-N logic returns expected chunks
  - [ ] Test prompt template formatting
  - [ ] Test token limit trimming

**Dependencies**: Vector store integration, LLM engine

---

### **Task 3: LLM Engine**
*Estimated: 3 hours | Timeout: 6 hours*

**Following TASKS.md: LLM Engine section**

- [ ] **Create `42/llm.py`** with Ollama wrapper
  - [ ] Implement `respond(prompt: str, stream: bool = False) -> str`
  - [ ] Connect to local Ollama server (default `mistral`)
  - [ ] Use Ollama HTTP API
  - [ ] Handle streaming responses
  - [ ] Add error handling for connection issues

- [ ] **Add streaming support** for CLI
  - [ ] Stream tokens back for progress feedback
  - [ ] Handle partial responses gracefully

- [ ] **Write tests** in `tests/test_llm.py`
  - [ ] Mock Ollama API responses
  - [ ] Test streaming functionality
  - [ ] Test error handling
  - [ ] Verify response quality

**Dependencies**: Ollama server running locally

---

### **Task 4: Enhanced Semantic Search**
*Estimated: 6 hours | Timeout: 12 hours*

**Following MASTERPLAN.md: Self-Tuning Embeddings vision**

- [ ] **Implement query embedding** in `auto_search`
  - [ ] Embed user queries into vector space
  - [ ] Use same embedding model as documents
  - [ ] Add to `_semantic_search_existing` method

- [ ] **Add hybrid retrieval** combining semantic + keyword
  - [ ] Search existing embeddings semantically
  - [ ] Combine with Brave API keyword search
  - [ ] Rerank results by relevance score
  - [ ] Implement query expansion from similar docs

- [ ] **Add semantic similarity search** to Brave API results
  - [ ] Filter search results by semantic similarity
  - [ ] Prioritize results that match mission objectives
  - [ ] Add relevance scoring for search results

- [ ] **Write tests** in `tests/test_semantic_search.py`
  - [ ] Test query embedding functionality
  - [ ] Test hybrid retrieval performance
  - [ ] Test semantic filtering accuracy

**Dependencies**: Embedding engine, vector store

---

### **Task 5: Mission Intelligence Enhancement**
*Estimated: 8 hours | Timeout: 16 hours*

**Following MASTERPLAN.md: Request & Missions, Conscience & Soul modules**

- [ ] **Enhance Moroni's mission analysis** in `42/un/moroni.py`
  - [ ] Add mission decomposition (break complex missions)
  - [ ] Implement query optimization based on existing knowledge
  - [ ] Add content strategy planning
  - [ ] Add learning orchestration

- [ ] **Add mission templates** for common scenarios
  - [ ] Research missions (academic, industry)
  - [ ] Monitoring missions (trends, updates)
  - [ ] Discovery missions (exploration, gaps)
  - [ ] Analysis missions (comparison, evaluation)

- [ ] **Implement mission chaining**
  - [ ] Link related missions automatically
  - [ ] Pass context between missions
  - [ ] Track mission dependencies

- [ ] **Add progress tracking**
  - [ ] Monitor mission completion rates
  - [ ] Track knowledge acquisition
  - [ ] Measure learning effectiveness

- [ ] **Write tests** in `tests/test_mission_intelligence.py`
  - [ ] Test mission decomposition
  - [ ] Test query optimization
  - [ ] Test mission chaining
  - [ ] Test progress tracking

**Dependencies**: LLM engine, semantic search

---

### **Task 6: Monitoring & Analytics**
*Estimated: 6 hours | Timeout: 12 hours*

**Following .cursor-rules: require_logging_for, performance_metrics**

- [ ] **Add embedding quality monitoring**
  - [ ] Track semantic similarity diversity
  - [ ] Monitor relevance score distribution
  - [ ] Alert on quality degradation
  - [ ] Generate quality reports

- [ ] **Create mission progress tracking**
  - [ ] Track mission completion rates
  - [ ] Monitor learning speed
  - [ ] Measure knowledge discovery rate
  - [ ] Generate progress reports

- [ ] **Implement learning effectiveness metrics**
  - [ ] Track self-learning improvements
  - [ ] Measure query optimization success
  - [ ] Monitor domain expertise growth
  - [ ] Generate learning reports

- [ ] **Build dashboard for system health**
  - [ ] Real-time system status
  - [ ] Performance metrics
  - [ ] Error rate monitoring
  - [ ] Resource usage tracking

- [ ] **Write tests** in `tests/test_monitoring.py`
  - [ ] Test quality metrics calculation
  - [ ] Test progress tracking accuracy
  - [ ] Test dashboard functionality

**Dependencies**: All previous tasks

---

## ðŸ”§ **Implementation Guidelines**

*Following .cursor-rules discipline:*

### **Code Quality**
- [ ] **Type hints**: All functions must have explicit type annotations
- [ ] **Formatting**: Use `black` for code formatting
- [ ] **Linting**: Use `ruff` for code quality
- [ ] **Documentation**: Add docstrings for all public functions

### **Error Handling**
- [ ] **Try/except**: For file I/O, network requests, vector DB ops
- [ ] **Logging**: Add logging for CLI execution and clustering operations
- [ ] **Timeout estimates**: Set realistic timeouts and cut functions if exceeded
- [ ] **Fail fast**: Handle errors gracefully with meaningful messages

### **Testing**
- [ ] **Unit tests**: Write tests for all new functionality
- [ ] **Integration tests**: Test component interactions
- [ ] **CLI tests**: Use typer testing utilities
- [ ] **API tests**: Use FastAPI TestClient

### **Performance**
- [ ] **Working prototypes**: Prioritize working code over perfect implementations
- [ ] **Timeout compliance**: Cut functions if they exceed estimated timeouts
- [ ] **Resource management**: Handle large embedding sets efficiently
- [ ] **Caching**: Reduce redundant computations

---

## ðŸ“Š **Success Metrics**

### **Technical Metrics**
- [ ] **Clustering Quality**: Meaningful document groups (silhouette score > 0.3)
- [ ] **Prompt Effectiveness**: Better LLM responses (relevance score > 0.7)
- [ ] **Semantic Search**: Improved query relevance (precision@5 > 0.6)
- [ ] **Mission Success**: Higher completion rates (> 80%)

### **Performance Metrics**
- [ ] **Processing Speed**: Embedding generation < 2 seconds per document
- [ ] **Memory Usage**: Handle 10k+ embeddings efficiently
- [ ] **Error Rate**: < 5% failure rate for core operations
- [ ] **Scalability**: Support concurrent mining operations

### **Quality Metrics**
- [ ] **Code Coverage**: > 80% test coverage
- [ ] **Documentation**: All public APIs documented
- [ ] **Linting**: Zero ruff violations
- [ ] **Type Safety**: All functions properly typed

---

## ðŸŽ¯ **Phase Progression**

### **Current: Phase un - Reflex and ingestion** âœ… COMPLETED
- Redis event bus working
- 42.un scanning for sources
- Task prioritization working
- Test suite in place

### **Next: Phase deux - Advanced optimization** ðŸš€ IN PROGRESS
- Bayesian optimization (BoTorch)
- Quantum solver (PennyLane + CUDA-Q)
- Knowledge graphs (NetworkX)
- Semantic compression via HDBSCAN

### **Future: Phase trois - Autonomous learning**
- Reinforcement loop via RLlib
- Synthetic data generation
- Contextual Memory Pruner

### **Future: Phase quatre - Multi-agent orchestration**
- Multiple specialized agents
- Dynamic memory pruning
- Orchestrator balancing compute

---

## ðŸš¨ **Critical Dependencies**

1. **Ollama server** must be running locally
2. **Qdrant** must be accessible and healthy
3. **Redis** must be running for event bus
4. **All pinned dependencies** must be installed correctly

## ðŸ“‹ **Task Execution Order**

1. **Task 1: Clustering Engine** (Foundation)
2. **Task 2: Prompt Builder** (Foundation)
3. **Task 3: LLM Engine** (Foundation)
4. **Task 4: Enhanced Semantic Search** (Enhancement)
5. **Task 5: Mission Intelligence Enhancement** (Advanced)
6. **Task 6: Monitoring & Analytics** (Monitoring)

*Each task builds on the previous ones and follows the established patterns from TASKS.md and MASTERPLAN.md.* 